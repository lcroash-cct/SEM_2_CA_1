{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT screenshots and/or terminal output to demonstrate the process of activating the hadoop cluster\n",
    "# then adding the files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hduser/env/myenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/usr/local/hadoop/bin:/bin:/usr/local/hadoop/sbin:/usr/local/hadoop/bin:/bin:/usr/local/hadoop/sbin:/opt/spark/bin:/usr/local/hadoop/bin:/bin:/usr/local/hadoop/sbin:/opt/spark/bin\n",
      "/home/hduser/env/myenv/bin/python\n",
      "/home/hduser/env/myenv\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import to_date, col\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# os.environ['SPARK_HOME']\n",
    "\n",
    "# os.environ['SPARK_HOME'] = '/opt/spark/'\n",
    "\n",
    "print(os.environ['PATH'])\n",
    "\n",
    "findspark.init('/opt/spark/')\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkExample').config('spark.driver.python', '/snap/jupyter/6/bin/python3').getOrCreate()\n",
    "\n",
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfs = spark.read.csv(\"hdfs://localhost:9000/transactions/*.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+---------+--------+-----+----------+------------+------------+------------+------------+--------------+-----+-----+---------------+-----------+--------------------+-----------+--------------------+----------+------------+------------+\n",
      "|      _c0|      _c1|         _c2|      _c3|     _c4|  _c5|       _c6|         _c7|         _c8|         _c9|        _c10|          _c11| _c12| _c13|           _c14|       _c15|                _c16|       _c17|                _c18|      _c19|        _c20|        _c21|\n",
      "+---------+---------+------------+---------+--------+-----+----------+------------+------------+------------+------------+--------------+-----+-----+---------------+-----------+--------------------+-----------+--------------------+----------+------------+------------+\n",
      "|SHOP_WEEK|SHOP_DATE|SHOP_WEEKDAY|SHOP_HOUR|QUANTITY|SPEND| PROD_CODE|PROD_CODE_10|PROD_CODE_20|PROD_CODE_30|PROD_CODE_40|     CUST_CODE|seg_1|seg_2|      BASKET_ID|BASKET_SIZE|BASKET_PRICE_SENS...|BASKET_TYPE|BASKET_DOMINANT_M...|STORE_CODE|STORE_FORMAT|STORE_REGION|\n",
      "|   200643| 20061224|           1|       14|       1| 1.77|PRD0900008|     CL00042|    DEP00011|      G00004|      D00002|CUST0000096000|   AZ|   BU|994103700265943|          M|                  UM| Small Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061218|           2|       19|       1| 1.05|PRD0900011|     CL00033|    DEP00008|      G00004|      D00002|CUST0000206560|   BG|   DI|994103700348637|          L|                  UM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061223|           7|       18|       3| 3.15|PRD0900011|     CL00033|    DEP00008|      G00004|      D00002|CUST0000017325|   CT|   AT|994103700207109|          L|                  MM|     Top Up|               Fresh|STORE00001|          LS|         E02|\n",
      "|   200643| 20061223|           7|       15|       1| 1.03|PRD0900015|     CL00015|    DEP00004|      G00003|      D00001|CUST0000871730|   BG| null|994103700845926|          L|                  UM|     Top Up|               Fresh|STORE00001|          LS|         E02|\n",
      "|   200643| 20061218|           2|       15|       3|13.26|PRD0900016|     CL00165|    DEP00055|      G00016|      D00003|CUST0000173993|   BG|   CZ|994103700324145|          L|                  MM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061219|           3|       12|       3|13.26|PRD0900016|     CL00165|    DEP00055|      G00016|      D00003|CUST0000540040|   CT|   AT|994103700597782|          L|                  MM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061219|           3|       15|       3|13.26|PRD0900016|     CL00165|    DEP00055|      G00016|      D00003|CUST0000017325|   CT|   AT|994103700207110|          L|                  MM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061218|           2|       15|       3| 2.55|PRD0900020|     CL00202|    DEP00067|      G00021|      D00005|CUST0000173993|   BG|   CZ|994103700324145|          L|                  MM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061224|           1|       19|       1| 0.85|PRD0900020|     CL00202|    DEP00067|      G00021|      D00005|          null| null| null|994103700017507|          L|                  MM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061223|           7|       15|       1| 0.52|PRD0900024|     CL00160|    DEP00054|      G00016|      D00003|CUST0000871730|   BG| null|994103700845926|          L|                  UM|     Top Up|               Fresh|STORE00001|          LS|         E02|\n",
      "|   200643| 20061222|           6|       12|       1| 0.52|PRD0900024|     CL00160|    DEP00054|      G00016|      D00003|CUST0000160899|   BG|   EQ|994103700314386|          L|                  MM|     Top Up|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061222|           6|        8|       3| 2.52|PRD0900034|     CL00129|    DEP00046|      G00013|      D00003|CUST0000534768|   AZ|   DI|994103700593728|          L|                  LA|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061219|           3|       21|       1| 1.56|PRD0900049|     CL00160|    DEP00054|      G00016|      D00003|CUST0000436394|   AZ| null|994103700520900|          L|                  MM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061218|           2|       13|       1| 1.75|PRD0900058|     CL00020|    DEP00005|      G00003|      D00001|CUST0000344309|   CT|   AT|994103700451904|          L|                  UM|  Full Shop|             Grocery|STORE00001|          LS|         E02|\n",
      "|   200643| 20061224|           1|       16|       1| 2.14|PRD0900063|     CL00185|    DEP00062|      G00018|      D00004|          null| null| null|994103700081186|          L|                  MM|     Top Up|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061222|           6|       15|       1| 1.09|PRD0900066|     CL00041|    DEP00011|      G00004|      D00002|CUST0000414514|   CT|   BU|994103700504485|          L|                  UM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061221|           5|        8|       1| 0.57|PRD0900069|     CL00134|    DEP00047|      G00013|      D00003|CUST0000622502|   AZ|   CZ|994103700659723|          L|                  MM|     Top Up|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061218|           2|       12|       1| 1.05|PRD0900077|     CL00150|    DEP00052|      G00015|      D00003|CUST0000710863|   AZ|   DI|994103700726239|          L|                  MM|  Full Shop|               Mixed|STORE00001|          LS|         E02|\n",
      "|   200643| 20061224|           1|       14|       1| 1.07|PRD0900079|     CL00157|    DEP00053|      G00016|      D00003|          null| null| null|994103700150926|          L|                  LA|     Top Up|               Mixed|STORE00001|          LS|         E02|\n",
      "+---------+---------+------------+---------+--------+-----+----------+------------+------------+------------+------------+--------------+-----+-----+---------------+-----------+--------------------+-----------+--------------------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SHOP_WEEK: string (nullable = true)\n",
      " |-- SHOP_DATE: string (nullable = true)\n",
      " |-- SHOP_WEEKDAY: string (nullable = true)\n",
      " |-- SHOP_HOUR: string (nullable = true)\n",
      " |-- QUANTITY: string (nullable = true)\n",
      " |-- SPEND: string (nullable = true)\n",
      " |-- PROD_CODE: string (nullable = true)\n",
      " |-- PROD_CODE_10: string (nullable = true)\n",
      " |-- PROD_CODE_20: string (nullable = true)\n",
      " |-- PROD_CODE_30: string (nullable = true)\n",
      " |-- PROD_CODE_40: string (nullable = true)\n",
      " |-- CUST_CODE: string (nullable = true)\n",
      " |-- seg_1: string (nullable = true)\n",
      " |-- seg_2: string (nullable = true)\n",
      " |-- BASKET_ID: string (nullable = true)\n",
      " |-- BASKET_SIZE: string (nullable = true)\n",
      " |-- BASKET_PRICE_SENSITIVITY: string (nullable = true)\n",
      " |-- BASKET_TYPE: string (nullable = true)\n",
      " |-- BASKET_DOMINANT_MISSION: string (nullable = true)\n",
      " |-- STORE_CODE: string (nullable = true)\n",
      " |-- STORE_FORMAT: string (nullable = true)\n",
      " |-- STORE_REGION: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform groupby operations to aggregate data by date for:\n",
    "##### total spend by store code\n",
    "##### total spend by store format\n",
    "##### total spend by prod code / prod_code_10 / prod_code_20 etc - depending on the highest level of granularity during EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"STORE_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df = spark.sql(\"SELECT SHOP_DATE, STORE_CODE, SUM(QUANTITY) AS TOTAL_QTY, ROUND(SUM(SPEND), 2) AS TOTAL_SPEND FROM STORE_DATA GROUP BY SHOP_DATE, STORE_CODE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 06:56:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 06:56:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----------+\n",
      "|SHOP_DATE|STORE_CODE|TOTAL_QTY|TOTAL_SPEND|\n",
      "+---------+----------+---------+-----------+\n",
      "| 20061223|STORE00003|    215.0|     316.65|\n",
      "| 20061219|STORE00113|    235.0|     340.55|\n",
      "| 20061221|STORE00120|     21.0|      70.53|\n",
      "| 20061224|STORE00227|     73.0|     164.17|\n",
      "| 20061224|STORE00432|     86.0|     118.75|\n",
      "| 20061219|STORE00494|     13.0|      10.78|\n",
      "| 20061221|STORE00633|    144.0|     228.09|\n",
      "| 20061221|STORE00639|    155.0|      178.8|\n",
      "| 20061222|STORE01091|      9.0|      12.73|\n",
      "| 20061220|STORE01215|    125.0|     202.03|\n",
      "| 20061218|STORE01289|     36.0|      37.31|\n",
      "| 20061222|STORE01330|     24.0|      22.07|\n",
      "| 20061222|STORE01603|     21.0|       27.4|\n",
      "| 20061224|STORE01651|    113.0|     137.57|\n",
      "| 20061222|STORE01672|     41.0|      53.55|\n",
      "| 20061221|STORE01707|    264.0|     350.76|\n",
      "| 20061223|STORE01793|     23.0|      18.85|\n",
      "| 20061223|STORE02524|    147.0|     200.77|\n",
      "| 20061219|STORE02575|      8.0|      28.24|\n",
      "| 20061220|STORE02619|    241.0|     316.43|\n",
      "+---------+----------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "store_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_format_df = spark.sql(\"SELECT SHOP_DATE, STORE_CODE, STORE_FORMAT, SUM(QUANTITY) AS TOTAL_QTY, ROUND(SUM(SPEND), 2) AS TOTAL_SPEND FROM STORE_DATA GROUP BY SHOP_DATE, STORE_CODE, STORE_FORMAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 06:58:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, STORE_CODE, STORE_FORMAT\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE, STORE_FORMAT\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 06:58:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE, STORE_FORMAT\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+---------+-----------+\n",
      "|SHOP_DATE|STORE_CODE|STORE_FORMAT|TOTAL_QTY|TOTAL_SPEND|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "| 20061218|STORE00107|          LS|     98.0|     153.96|\n",
      "| 20061218|STORE00120|          MS|      8.0|      22.61|\n",
      "| 20061218|STORE00422|          MS|     78.0|     147.67|\n",
      "| 20061221|STORE00425|          MS|     63.0|     118.09|\n",
      "| 20061221|STORE00460|          LS|    217.0|     317.46|\n",
      "| 20061221|STORE00807|          LS|     66.0|      79.44|\n",
      "| 20061222|STORE00840|          LS|    325.0|     487.22|\n",
      "| 20061221|STORE00877|          LS|    290.0|     487.31|\n",
      "| 20061223|STORE00895|          SS|      6.0|       8.73|\n",
      "| 20061222|STORE00966|          MS|    145.0|     194.21|\n",
      "| 20061218|STORE01035|          LS|     16.0|      16.61|\n",
      "| 20061219|STORE01160|          LS|    108.0|     136.29|\n",
      "| 20061224|STORE01298|          SS|     26.0|      26.28|\n",
      "| 20061221|STORE01316|          LS|    245.0|     429.21|\n",
      "| 20061221|STORE01501|          MS|     90.0|     113.72|\n",
      "| 20061218|STORE01535|          SS|      3.0|       2.27|\n",
      "| 20061220|STORE01650|          SS|     31.0|      81.67|\n",
      "| 20061223|STORE01695|          LS|    224.0|     483.26|\n",
      "| 20061219|STORE01984|          SS|      3.0|       2.21|\n",
      "| 20061218|STORE02192|          SS|      5.0|       4.71|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "store_format_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_10_df = spark.sql(\"SELECT SHOP_DATE, STORE_CODE, PROD_CODE_10, SUM(QUANTITY) AS TOTAL_QTY, ROUND(SUM(SPEND), 2) AS TOTAL_SPEND FROM STORE_DATA GROUP BY SHOP_DATE, STORE_CODE, PROD_CODE_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:03:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, PROD_CODE_10, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_10, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:03:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_10, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+---------+-----------+\n",
      "|SHOP_DATE|STORE_CODE|PROD_CODE_10|TOTAL_QTY|TOTAL_SPEND|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "| 20060410|STORE00001|     CL00012|      1.0|       0.01|\n",
      "| 20060410|STORE00001|     CL00020|      5.0|        2.2|\n",
      "| 20060410|STORE00001|     CL00201|      1.0|       0.81|\n",
      "| 20060410|STORE00002|     CL00001|      2.0|       1.72|\n",
      "| 20060410|STORE00003|     CL00023|      1.0|       1.93|\n",
      "| 20060410|STORE00003|     CL00065|      1.0|       1.16|\n",
      "| 20060410|STORE00003|     CL00074|      2.0|       5.22|\n",
      "| 20060410|STORE00003|     CL00128|      1.0|       1.02|\n",
      "| 20060410|STORE00003|     CL00135|      1.0|       0.16|\n",
      "| 20060410|STORE00003|     CL00157|      2.0|       1.58|\n",
      "| 20060410|STORE00003|     CL00161|      3.0|        3.3|\n",
      "| 20060410|STORE00004|     CL00017|      1.0|       3.35|\n",
      "| 20060410|STORE00004|     CL00043|      1.0|       0.97|\n",
      "| 20060410|STORE00004|     CL00063|      6.0|       6.64|\n",
      "| 20060410|STORE00004|     CL00070|      3.0|       0.84|\n",
      "| 20060410|STORE00004|     CL00083|      3.0|       6.42|\n",
      "| 20060410|STORE00004|     CL00128|      1.0|        0.9|\n",
      "| 20060410|STORE00004|     CL00159|      2.0|       1.94|\n",
      "| 20060410|STORE00006|     CL00068|      1.0|       2.19|\n",
      "| 20060410|STORE00006|     CL00090|      3.0|       8.64|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prod_10_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_20_df = spark.sql(\"SELECT SHOP_DATE, STORE_CODE, PROD_CODE_20, SUM(QUANTITY) AS TOTAL_QTY, ROUND(SUM(SPEND), 2) AS TOTAL_SPEND FROM STORE_DATA GROUP BY SHOP_DATE, STORE_CODE, PROD_CODE_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:05:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, PROD_CODE_20, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_20, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:05:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_20, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+---------+-----------+\n",
      "|SHOP_DATE|STORE_CODE|PROD_CODE_20|TOTAL_QTY|TOTAL_SPEND|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "| 20060410|STORE00001|    DEP00003|      1.0|       0.01|\n",
      "| 20060410|STORE00001|    DEP00005|      5.0|        2.2|\n",
      "| 20060410|STORE00001|    DEP00025|      3.0|       2.61|\n",
      "| 20060410|STORE00001|    DEP00051|      3.0|       3.97|\n",
      "| 20060410|STORE00001|    DEP00067|      1.0|       0.81|\n",
      "| 20060410|STORE00001|    DEP00070|      1.0|       0.67|\n",
      "| 20060410|STORE00002|    DEP00008|      2.0|       2.21|\n",
      "| 20060410|STORE00002|    DEP00011|      1.0|        0.4|\n",
      "| 20060410|STORE00003|    DEP00008|     13.0|      17.66|\n",
      "| 20060410|STORE00003|    DEP00016|      1.0|       5.14|\n",
      "| 20060410|STORE00003|    DEP00021|     11.0|      16.59|\n",
      "| 20060410|STORE00003|    DEP00025|      4.0|       4.83|\n",
      "| 20060410|STORE00003|    DEP00069|      1.0|       1.87|\n",
      "| 20060410|STORE00003|    DEP00073|      3.0|       1.87|\n",
      "| 20060410|STORE00003|    DEP00083|      1.0|       6.26|\n",
      "| 20060410|STORE00004|    DEP00008|      4.0|       5.88|\n",
      "| 20060410|STORE00004|    DEP00010|      1.0|       0.43|\n",
      "| 20060410|STORE00004|    DEP00013|      2.0|      20.99|\n",
      "| 20060410|STORE00004|    DEP00021|      3.0|       5.21|\n",
      "| 20060410|STORE00004|    DEP00022|      4.0|       4.62|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prod_20_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_30_df = spark.sql(\"SELECT SHOP_DATE, STORE_CODE, PROD_CODE_30, SUM(QUANTITY) AS TOTAL_QTY, ROUND(SUM(SPEND), 2) AS TOTAL_SPEND FROM STORE_DATA GROUP BY SHOP_DATE, STORE_CODE, PROD_CODE_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:07:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, PROD_CODE_30, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_30, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:07:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_30, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+---------+-----------+\n",
      "|SHOP_DATE|STORE_CODE|PROD_CODE_30|TOTAL_QTY|TOTAL_SPEND|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "| 20060410|STORE00001|      G00002|      1.0|       0.01|\n",
      "| 20060410|STORE00001|      G00003|      5.0|        2.2|\n",
      "| 20060410|STORE00001|      G00006|      1.0|       2.58|\n",
      "| 20060410|STORE00001|      G00010|      3.0|       1.02|\n",
      "| 20060410|STORE00001|      G00015|      4.0|       5.26|\n",
      "| 20060410|STORE00002|      G00003|      1.0|       2.05|\n",
      "| 20060410|STORE00002|      G00011|      1.0|       0.41|\n",
      "| 20060410|STORE00002|      G00016|      3.0|       3.64|\n",
      "| 20060410|STORE00002|      G00021|      1.0|        1.4|\n",
      "| 20060410|STORE00002|      G00023|      1.0|       0.31|\n",
      "| 20060410|STORE00003|      G00002|      2.0|       2.11|\n",
      "| 20060410|STORE00003|      G00003|      2.0|        2.2|\n",
      "| 20060410|STORE00003|      G00004|     20.0|      23.56|\n",
      "| 20060410|STORE00003|      G00007|     47.0|      67.46|\n",
      "| 20060410|STORE00003|      G00013|      2.0|       1.18|\n",
      "| 20060410|STORE00003|      G00015|      4.0|       5.89|\n",
      "| 20060410|STORE00004|      G00003|      1.0|       3.35|\n",
      "| 20060410|STORE00004|      G00004|      8.0|       8.69|\n",
      "| 20060410|STORE00004|      G00008|      1.0|       0.87|\n",
      "| 20060410|STORE00004|      G00015|      1.0|       0.93|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prod_30_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_40_df = spark.sql(\"SELECT SHOP_DATE, STORE_CODE, PROD_CODE_40, SUM(QUANTITY) AS TOTAL_QTY, ROUND(SUM(SPEND), 2) AS TOTAL_SPEND FROM STORE_DATA GROUP BY SHOP_DATE, STORE_CODE, PROD_CODE_40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:09:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, PROD_CODE_40, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_40, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:09:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_40, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+---------+-----------+\n",
      "|SHOP_DATE|STORE_CODE|PROD_CODE_40|TOTAL_QTY|TOTAL_SPEND|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "| 20061219|STORE00027|      D00002|     40.0|      58.04|\n",
      "| 20061218|STORE00030|      D00001|      7.0|       8.51|\n",
      "| 20061221|STORE00065|      D00004|      4.0|       7.92|\n",
      "| 20061218|STORE00327|      D00002|     39.0|      59.88|\n",
      "| 20061222|STORE00327|      D00002|     52.0|      77.29|\n",
      "| 20061219|STORE00337|      D00001|      4.0|       4.88|\n",
      "| 20061223|STORE00352|      D00003|      7.0|       5.06|\n",
      "| 20061218|STORE00377|      D00002|     84.0|     114.26|\n",
      "| 20061224|STORE00377|      D00004|      2.0|       4.18|\n",
      "| 20061219|STORE00468|      D00001|      2.0|       2.25|\n",
      "| 20061218|STORE00500|      D00009|      1.0|       1.35|\n",
      "| 20061223|STORE00508|      D00001|      5.0|       2.16|\n",
      "| 20061224|STORE00560|      D00003|     16.0|       11.8|\n",
      "| 20061219|STORE00574|      D00001|     23.0|      16.83|\n",
      "| 20061223|STORE00611|      D00002|     64.0|      86.44|\n",
      "| 20061224|STORE00616|      D00003|     27.0|      19.29|\n",
      "| 20061220|STORE00625|      D00001|      9.0|       5.01|\n",
      "| 20061224|STORE00667|      D00008|      9.0|       63.0|\n",
      "| 20061224|STORE00673|      D00001|      5.0|       7.55|\n",
      "| 20061218|STORE00704|      D00001|      4.0|        4.3|\n",
      "+---------+----------+------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prod_40_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SHOP_DATE: string (nullable = true)\n",
      " |-- STORE_CODE: string (nullable = true)\n",
      " |-- PROD_CODE_40: string (nullable = true)\n",
      " |-- TOTAL_QTY: double (nullable = true)\n",
      " |-- TOTAL_SPEND: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_40_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the estimated size of each of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_df_memory_usage(df):\n",
    "    sample = df.sample(fraction=0.01)\n",
    "    pdf = sample.toPandas()\n",
    "    print(pdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:34:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:34:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6057 entries, 0 to 6056\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   SHOP_DATE    6057 non-null   object \n",
      " 1   STORE_CODE   6056 non-null   object \n",
      " 2   TOTAL_QTY    6056 non-null   float64\n",
      " 3   TOTAL_SPEND  6056 non-null   float64\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 189.4+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "estimate_df_memory_usage(store_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated df size = 189.4 kb * 100 = 18.9MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:36:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, STORE_CODE, STORE_FORMAT\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE, STORE_FORMAT\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:36:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE, STORE_FORMAT\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "[Stage 42:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6005 entries, 0 to 6004\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   SHOP_DATE     6005 non-null   object \n",
      " 1   STORE_CODE    6004 non-null   object \n",
      " 2   STORE_FORMAT  6004 non-null   object \n",
      " 3   TOTAL_QTY     6004 non-null   float64\n",
      " 4   TOTAL_SPEND   6004 non-null   float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 234.7+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "estimate_df_memory_usage(store_format_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated df size = 235kb * 100 = 23.5MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:39:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, PROD_CODE_10, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_10, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:39:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_10, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 212111 entries, 0 to 212110\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   SHOP_DATE     212111 non-null  object \n",
      " 1   STORE_CODE    212109 non-null  object \n",
      " 2   PROD_CODE_10  212109 non-null  object \n",
      " 3   TOTAL_QTY     212109 non-null  float64\n",
      " 4   TOTAL_SPEND   212109 non-null  float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 8.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "estimate_df_memory_usage(prod_10_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated df size = 8.1MB * 100 = 810MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:42:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, PROD_CODE_20, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_20, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:42:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_20, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 136880 entries, 0 to 136879\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   SHOP_DATE     136880 non-null  object \n",
      " 1   STORE_CODE    136879 non-null  object \n",
      " 2   PROD_CODE_20  136879 non-null  object \n",
      " 3   TOTAL_QTY     136879 non-null  float64\n",
      " 4   TOTAL_SPEND   136879 non-null  float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 5.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "estimate_df_memory_usage(prod_20_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated df size = 5.2MB * 100 = 520MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:44:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, PROD_CODE_30, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_30, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:44:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_30, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77700 entries, 0 to 77699\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   SHOP_DATE     77700 non-null  object \n",
      " 1   STORE_CODE    77698 non-null  object \n",
      " 2   PROD_CODE_30  77698 non-null  object \n",
      " 3   TOTAL_QTY     77698 non-null  float64\n",
      " 4   TOTAL_SPEND   77698 non-null  float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 3.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "estimate_df_memory_usage(prod_30_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated df size = 3MB * 100 = 300MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 07:31:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, PROD_CODE_40, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_40, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/12 07:31:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, PROD_CODE_40, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28393 entries, 0 to 28392\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   SHOP_DATE     28393 non-null  object \n",
      " 1   STORE_CODE    28393 non-null  object \n",
      " 2   PROD_CODE_40  28393 non-null  object \n",
      " 3   TOTAL_QTY     28393 non-null  float64\n",
      " 4   TOTAL_SPEND   28393 non-null  float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "estimate_df_memory_usage(prod_40_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated df size = 1.1MB * 100 = 110MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert string fields to numeric by extracting the numeric values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_convert_to_numeric(code):\n",
    "    numeric_part = re.search(r'\\d+$', code).group()\n",
    "    return int(numeric_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_col_to_date(df, cols):\n",
    "    for col_name in cols:\n",
    "        df_date = df.withColumn(col_name, to_date(col(col_name), \"yyyyMMdd\"))\n",
    "    return df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the UDF for use with the dataframe\n",
    "extract_and_convert_to_numeric_udf = udf(extract_and_convert_to_numeric, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_col_to_int(df, cols):\n",
    "    for col_name in cols:\n",
    "        df_int = df.withColumn(col_name, extract_and_convert_to_numeric_udf(df[col_name]))\n",
    "        # df_int.drop(col_name)\n",
    "    return df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df_int = convert_string_col_to_int(store_df, [\"STORE_CODE\"])\n",
    "store_df_final = convert_string_col_to_date(store_df_int, [\"SHOP_DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SHOP_DATE: string (nullable = true)\n",
      " |-- STORE_CODE: integer (nullable = true)\n",
      " |-- TOTAL_QTY: double (nullable = true)\n",
      " |-- TOTAL_SPEND: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store_df_int.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 08:46:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/13 08:46:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "[Stage 72:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-----------+\n",
      "|SHOP_DATE|STORE_CODE|TOTAL_QTY|TOTAL_SPEND|\n",
      "+---------+----------+---------+-----------+\n",
      "| 20061223|         3|    215.0|     316.65|\n",
      "| 20061219|       113|    235.0|     340.55|\n",
      "| 20061221|       120|     21.0|      70.53|\n",
      "| 20061224|       227|     73.0|     164.17|\n",
      "| 20061224|       432|     86.0|     118.75|\n",
      "| 20061219|       494|     13.0|      10.78|\n",
      "| 20061221|       633|    144.0|     228.09|\n",
      "| 20061221|       639|    155.0|      178.8|\n",
      "| 20061222|      1091|      9.0|      12.73|\n",
      "| 20061220|      1215|    125.0|     202.03|\n",
      "| 20061218|      1289|     36.0|      37.31|\n",
      "| 20061222|      1330|     24.0|      22.07|\n",
      "| 20061222|      1603|     21.0|       27.4|\n",
      "| 20061224|      1651|    113.0|     137.57|\n",
      "| 20061222|      1672|     41.0|      53.55|\n",
      "| 20061221|      1707|    264.0|     350.76|\n",
      "| 20061223|      1793|     23.0|      18.85|\n",
      "| 20061223|      2524|    147.0|     200.77|\n",
      "| 20061219|      2575|      8.0|      28.24|\n",
      "| 20061220|      2619|    241.0|     316.43|\n",
      "+---------+----------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "store_df_int.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SHOP_DATE: date (nullable = true)\n",
      " |-- STORE_CODE: integer (nullable = true)\n",
      " |-- TOTAL_QTY: double (nullable = true)\n",
      " |-- TOTAL_SPEND: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store_df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 08:49:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/13 08:49:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+-----------+\n",
      "| SHOP_DATE|STORE_CODE|TOTAL_QTY|TOTAL_SPEND|\n",
      "+----------+----------+---------+-----------+\n",
      "|2006-12-23|         3|    215.0|     316.65|\n",
      "|2006-12-19|       113|    235.0|     340.55|\n",
      "|2006-12-21|       120|     21.0|      70.53|\n",
      "|2006-12-24|       227|     73.0|     164.17|\n",
      "|2006-12-24|       432|     86.0|     118.75|\n",
      "|2006-12-19|       494|     13.0|      10.78|\n",
      "|2006-12-21|       633|    144.0|     228.09|\n",
      "|2006-12-21|       639|    155.0|      178.8|\n",
      "|2006-12-22|      1091|      9.0|      12.73|\n",
      "|2006-12-20|      1215|    125.0|     202.03|\n",
      "|2006-12-18|      1289|     36.0|      37.31|\n",
      "|2006-12-22|      1330|     24.0|      22.07|\n",
      "|2006-12-22|      1603|     21.0|       27.4|\n",
      "|2006-12-24|      1651|    113.0|     137.57|\n",
      "|2006-12-22|      1672|     41.0|      53.55|\n",
      "|2006-12-21|      1707|    264.0|     350.76|\n",
      "|2006-12-23|      1793|     23.0|      18.85|\n",
      "|2006-12-23|      2524|    147.0|     200.77|\n",
      "|2006-12-19|      2575|      8.0|      28.24|\n",
      "|2006-12-20|      2619|    241.0|     316.43|\n",
      "+----------+----------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "store_df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore EDA capabilities of pyspark - is it better to use pandas on the data directly?\n",
    "### How useful is spark for feature engineering vs python?\n",
    "### Is it better to build NN in spark (does spark support keras/tensorflow) or in python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SHOP_DATE: date, STORE_CODE: int, TOTAL_QTY: double, TOTAL_SPEND: double]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_df_final.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 09:17:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, \n",
      " Schema: SHOP_DATE, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "610245"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 09:20:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: SHOP_DATE, QUANTITY, SFNND, STORE_CODE\n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SPEND but found: SFNND\n",
      "CSV file: hdfs://localhost:9000/transactions/transactions_200607.csv\n",
      "23/09/13 09:20:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: date_from, , , \n",
      " Schema: SHOP_DATE, QUANTITY, SPEND, STORE_CODE\n",
      "Expected: SHOP_DATE but found: date_from\n",
      "CSV file: hdfs://localhost:9000/transactions/time.csv\n",
      "23/09/13 09:20:41 ERROR Executor: Exception in task 0.0 in stage 91.0 (TID 1545)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_29471/740512394.py\", line 2, in extract_and_convert_to_numeric\n",
      "  File \"/usr/lib/python3.10/re.py\", line 200, in search\n",
      "    return _compile(pattern, flags).search(string)\n",
      "TypeError: expected string or bytes-like object\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/13 09:20:41 WARN TaskSetManager: Lost task 0.0 in stage 91.0 (TID 1545) (10.0.2.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_29471/740512394.py\", line 2, in extract_and_convert_to_numeric\n",
      "  File \"/usr/lib/python3.10/re.py\", line 200, in search\n",
      "    return _compile(pattern, flags).search(string)\n",
      "TypeError: expected string or bytes-like object\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/09/13 09:20:41 ERROR TaskSetManager: Task 0 in stage 91.0 failed 1 times; aborting job\n",
      "23/09/13 09:20:41 WARN TaskSetManager: Lost task 1.0 in stage 91.0 (TID 1546) (10.0.2.15 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_29471/740512394.py\", line 2, in extract_and_convert_to_numeric\n  File \"/usr/lib/python3.10/re.py\", line 200, in search\n    return _compile(pattern, flags).search(string)\nTypeError: expected string or bytes-like object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestimate_df_memory_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_df_final\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 3\u001b[0m, in \u001b[0;36mestimate_df_memory_usage\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mestimate_df_memory_usage\u001b[39m(df):\n\u001b[1;32m      2\u001b[0m     sample \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pdf\u001b[38;5;241m.\u001b[39minfo())\n",
      "File \u001b[0;32m~/env/myenv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/env/myenv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/env/myenv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/env/myenv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_29471/740512394.py\", line 2, in extract_and_convert_to_numeric\n  File \"/usr/lib/python3.10/re.py\", line 200, in search\n    return _compile(pattern, flags).search(string)\nTypeError: expected string or bytes-like object\n"
     ]
    }
   ],
   "source": [
    "estimate_df_memory_usage(store_df_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
